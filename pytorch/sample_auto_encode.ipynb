{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from uility import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "trainx=pd.read_csv('../data/x_1730_1024_3.csv')\n",
    "trainy=pd.read_csv('../data/y_1730_1024_3.csv')\n",
    "#trainx=pd.read_csv('../data/x_600_2_JN.csv')\n",
    "#trainy=pd.read_csv('../data/y_600_2_JN.csv')\n",
    "#x_test=pd.read_csv('../data/x_1772_1024_3.csv')\n",
    "#y_test=pd.read_csv('../data/y_1772_1024_3.csv')\n",
    "index =list(trainx.index)\n",
    "random.Random(0).shuffle(index)\n",
    "num=int(len(index)/5*3)\n",
    "x_train,y_train=trainx.iloc[index[:num],:].values,trainy.iloc[index[:num],:].values\n",
    "x_test,y_test=trainx.iloc[index[num:],:].values,trainy.iloc[index[num:],:].values\n",
    "data_X=np.vstack([x_train,x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1024\n",
    "H1=400\n",
    "H2=200\n",
    "H3=100\n",
    "drop_rate=0.1\n",
    "out_dim=10\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super( Encoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(N, H1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(H1),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(H1,H2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(H2),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(H2, H3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(H3),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super( Decoder,self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(H3,H2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(H2,H1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H1,N),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        decoded = self.decoder(x)\n",
    "        return decoded\n",
    "class Soft_class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(H3,out_dim),)\n",
    "    \n",
    "    def forward(self,x):   \n",
    "        x=F.log_softmax(self.fc(x),dim=1)\n",
    "        return x\n",
    "encoder=Encoder()\n",
    "decoder=Decoder()\n",
    "soft_class=Soft_class()\n",
    "optimizer_en = torch.optim.Adam(encoder.parameters(), lr=1e-3,weight_decay=0.01)\n",
    "optimizer_de = torch.optim.Adam(decoder.parameters(), lr=1e-3,weight_decay=0.01)\n",
    "optimizer_sf = torch.optim.Adam(soft_class.parameters(), lr=1e-3,weight_decay=0.01)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.0898\n",
      "Epoch:  1 | train loss: 0.0788\n",
      "Epoch:  2 | train loss: 0.0788\n",
      "Epoch:  3 | train loss: 0.0788\n",
      "Epoch:  4 | train loss: 0.0788\n",
      "Epoch:  5 | train loss: 0.0788\n",
      "Epoch:  6 | train loss: 0.0788\n",
      "Epoch:  7 | train loss: 0.0788\n",
      "Epoch:  8 | train loss: 0.0788\n",
      "Epoch:  9 | train loss: 0.0788\n"
     ]
    }
   ],
   "source": [
    "## 自编码\n",
    "\n",
    "batch_size=128\n",
    "EPOCH=10\n",
    "for epoch in range(EPOCH):\n",
    "    train_step=int(len(x_train)/batch_size)\n",
    "    generate_data=get_batch(x_train,x_train,batch_size)\n",
    "    for step in range(train_step):\n",
    "        bx_,by_ =next(generate_data)\n",
    "        bx=Variable(torch.Tensor(bx_))\n",
    "        by=Variable(torch.Tensor(by_))     \n",
    "        encoded =encoder(bx)        \n",
    "        decoded=decoder(encoded)\n",
    "        loss = loss_func(decoded, by)\n",
    "        optimizer_de.zero_grad()\n",
    "        optimizer_en.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_de.step()\n",
    "        optimizer_en.step()\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: ', epoch,'| train loss: %.4f' % loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.0751\n",
      "Epoch:  1 | train loss: 0.0622\n",
      "Epoch:  2 | train loss: 0.0807\n",
      "Epoch:  3 | train loss: 0.0464\n",
      "Epoch:  4 | train loss: 0.0619\n",
      "Epoch:  5 | train loss: 0.0633\n",
      "Epoch:  6 | train loss: 0.0537\n",
      "Epoch:  7 | train loss: 0.0831\n",
      "Epoch:  8 | train loss: 0.0931\n",
      "Epoch:  9 | train loss: 0.0509\n",
      "Epoch:  10 | train loss: 0.0448\n",
      "Epoch:  11 | train loss: 0.0475\n",
      "Epoch:  12 | train loss: 0.0474\n",
      "Epoch:  13 | train loss: 0.0837\n",
      "Epoch:  14 | train loss: 0.1495\n",
      "Epoch:  15 | train loss: 0.0475\n",
      "Epoch:  16 | train loss: 0.0507\n",
      "Epoch:  17 | train loss: 0.0429\n",
      "Epoch:  18 | train loss: 0.0881\n",
      "Epoch:  19 | train loss: 0.0570\n",
      "Epoch:  20 | train loss: 0.0627\n",
      "Epoch:  21 | train loss: 0.0492\n",
      "Epoch:  22 | train loss: 0.1045\n",
      "Epoch:  23 | train loss: 0.0539\n",
      "Epoch:  24 | train loss: 0.0458\n",
      "Epoch:  25 | train loss: 0.0756\n",
      "Epoch:  26 | train loss: 0.1052\n",
      "Epoch:  27 | train loss: 0.0706\n",
      "Epoch:  28 | train loss: 0.0727\n",
      "Epoch:  29 | train loss: 0.0750\n",
      "Epoch:  30 | train loss: 0.0426\n",
      "Epoch:  31 | train loss: 0.0298\n",
      "Epoch:  32 | train loss: 0.0367\n",
      "Epoch:  33 | train loss: 0.0588\n",
      "Epoch:  34 | train loss: 0.0532\n",
      "Epoch:  35 | train loss: 0.0859\n",
      "Epoch:  36 | train loss: 0.0831\n",
      "Epoch:  37 | train loss: 0.0581\n",
      "Epoch:  38 | train loss: 0.0522\n",
      "Epoch:  39 | train loss: 0.0388\n",
      "Epoch:  40 | train loss: 0.0563\n",
      "Epoch:  41 | train loss: 0.0877\n",
      "Epoch:  42 | train loss: 0.0601\n",
      "Epoch:  43 | train loss: 0.0376\n",
      "Epoch:  44 | train loss: 0.0432\n",
      "Epoch:  45 | train loss: 0.0514\n",
      "Epoch:  46 | train loss: 0.0525\n",
      "Epoch:  47 | train loss: 0.0639\n",
      "Epoch:  48 | train loss: 0.0677\n",
      "Epoch:  49 | train loss: 0.0625\n"
     ]
    }
   ],
   "source": [
    "#分类\n",
    "batch_size=128\n",
    "EPOCH=50\n",
    "for epoch in range(EPOCH):\n",
    "    train_step=int(len(x_train)/batch_size)\n",
    "    generate_data=get_batch(x_train,y_train,batch_size)\n",
    "    for step in range(train_step):\n",
    "        bx_,by_ =next(generate_data)\n",
    "        bx=Variable(torch.Tensor(bx_))\n",
    "        by=Variable(torch.Tensor(by_.argmax(axis=1))).long()\n",
    "        \n",
    "        encoded =encoder(bx)        \n",
    "        soft_=soft_class(encoded)\n",
    "        loss = F.nll_loss(soft_,by)\n",
    "        \n",
    "        optimizer_sf.zero_grad()\n",
    "        optimizer_en.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_sf.step()\n",
    "        optimizer_en.step()\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0.1834402, dtype=float32), 0.9534632034632035)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  eval(x_test,y_test):\n",
    "    x_test_=Variable(torch.Tensor(x_test))\n",
    "    y_test_=Variable(torch.Tensor(y_test.argmax(axis=1))).long()\n",
    "    encoded =encoder(x_test_)\n",
    "    soft_=soft_class(encoded)\n",
    "    acc=(soft_.argmax(axis=1)==y_test_).sum().numpy()/len(y_test)\n",
    "    cost= F.nll_loss(soft_,y_test_).detach().numpy()\n",
    "    return cost,acc\n",
    "eval(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a=autoencoder.state_dict()\n",
    "model_dict.update(pretrained_dict)\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "cnn.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
